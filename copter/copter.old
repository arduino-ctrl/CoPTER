import argparse
import os
from ns3gym import ns3env
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import random
from loguru import logger

from agent import CopterAgent
from utils import ReplayBuffer, cal_reward
from fmaps.fmaps import import_fmaps

from parameters import CopterParameters, DCQCNParameters

if __name__ == "__main__":
    # Initial Logger
    logger.add("log/copter_{time}.log", level="INFO", rotation="5 MB")
    logger.info(f"Copter Starting Running at {os.getcwd()}")
    # Hyper-parameters
    # COPTER_N_SWITCH_STATE = 6
    # COPTER_N_SWITCH_HISTORY = 3
    # # COPTER_N_ACTION_DIM = 3
    # COPTER_N_ACTION_DIM = 2  # We don't tune Pmax now.

    # Initialize ns3-gym environment
    port = 5555
    # env = ns3env.Ns3Env(port=port, startSim=False)
    # env.reset()

    # # Get observation and action space information
    # ob_space = env.observation_space
    # ac_space = env.action_space
    # logger.info(f"Observation space: {ob_space} -> shape = {ob_space.shape}")
    # logger.info(f"Action space: {ac_space}")

    # # Calculation Switch Objects
    # n_port = ob_space.shape[0] // COPTER_N_SWITCH_STATE
    # logger.info(f"Number of switch ports: {n_port}")

    # # History observation queue with fixed length
    # obs_history = [deque(maxlen=COPTER_N_SWITCH_HISTORY) for _ in range(n_port)]


    # Training Parameters
    training_interval = 4
    target_update_interval = 8
    reward_gamma = 0.95
    batch_size = 512
    batch_size_single_port = 16
    memory_size = 2560
    memory_size_single_port = 64

    # Initialize global memory and Copter Agent
    state_dim = COPTER_N_SWITCH_STATE * COPTER_N_SWITCH_HISTORY
    num_actions = 10
    copter_function = import_fmaps("fmaps/WebServer_Load0.7_avg.fmap")

    # Agent settings:
    COPTER_CENTRALIZED = False

    if COPTER_CENTRALIZED:
        global_memory = ReplayBuffer(capacity=memory_size)
        agent = CopterAgent(state_dim, num_actions, copter_function, replay_buffer=global_memory, batch_size=batch_size, gamma=reward_gamma, target_update_steps=target_update_interval)
    else:
        agent_pool:list[CopterAgent] = []
        for i in range(n_port):
            agent_pool.append(CopterAgent(state_dim, num_actions, copter_function, local_buffer_size=memory_size_single_port, batch_size=batch_size_single_port, gamma=reward_gamma, target_update_steps=target_update_interval))


    # States
    step_index = 0

    try:
        while True:
            # logger.info("Starting Monitoring the Switch Status...")

            # obs = env.reset()
            # obs = np.array(obs)

            # logger.info(f"Step {step_index} - Observation {obs.shape}: {obs}")

            # # Initialize observation history
            # for i in range(n_port):
            #     obs_history[i].append(obs[i*6:(i+1)*6]) # obs_history = [[obs_0^0, ], [obs_1^0, ], ..., [obs_n^0, ]]

            while True:
                action_list = []
                q_action_list = []
                for i in range(n_port):
                    if len(obs_history[i]) == 3:
                        # Make a decision ...
                        step_state_i = np.concatenate(obs_history[i])
                        if COPTER_CENTRALIZED:
                            step_action_i, q_action = agent.select_action(step_state_i, norm=True)
                        else:
                            step_action_i, q_action = agent_pool[i].select_action(step_state_i, norm=True)
                        action_list.append(np.array([step_action_i[0], step_action_i[1], obs_history[i][-1][-1]]))
                        q_action_list.append(q_action)
                    else:
                        action_list.append(np.array(obs_history[i][-1][-3:]))
                        # Not enough observation to make a decision, just keep the parameters...
                action = np.concatenate(action_list)


                logger.info(f"Step {step_index} - Action {action.shape}: {action}")
                if len(q_action_list) > 0:
                    q_agent = np.array(q_action_list)
                    logger.info(f"Step {step_index} - Action Q Average = {np.average(q_agent)}")

                # Executate the decision and get next observation
                next_obs, _, done, info = env.step(action)  # ns-3 environment doesn't return reward, we calculate it in Python side instead.
                next_obs = np.array(next_obs)
                next_obs_list = [next_obs[i*6:(i+1)*6] for i in range(n_port)]
                logger.info(f"Step {step_index} - Next Observation {next_obs.shape}: {next_obs}")

                # TODO: refine reward calculation for distributed mode
                # Calculation reward in Python side with next observation
                reward = cal_reward(next_obs_list, n_port)
                logger.info(f"Step {step_index} - Average Reward = {np.average(reward)}")

                # Now we finally get `next_obs` and `reward`, so we can update the Replay Buffer for training
                for i in range(n_port):
                    if len(obs_history[i]) == 3:
                        step_state_i = np.concatenate(obs_history[i])
                        next_state_i = np.concatenate([obs_history[i][1], obs_history[i][2], next_obs_list[i]])  # obs_history[i][0] will be replaced by new observation.
                        if COPTER_CENTRALIZED:
                            global_memory.push(step_state_i, action_list[i], reward[i], next_state_i, False)
                        else:
                            agent_pool[i].buffer.push(step_state_i, action_list[i], reward[i], next_state_i, False)
                
                logger.info(f"Step {step_index} - Buffer Updated.")


                for i in range(n_port):
                    obs_history[i].append(next_obs_list[i])
                
                if step_index % training_interval == 0:
                    if COPTER_CENTRALIZED:
                        agent.update_model()
                        agent.save_model()
                    else:
                        agent_pool[i].update_model()
                        # We don't save model for distributed copter/ACC model, but we will load model from centralized training results.
                        

                step_index += 1
                agent.update_step(step_index)
                if done:
                    break

    except KeyboardInterrupt:
        print("Ctrl-C -> Exit")
    finally:
        env.close()
        print("Done")